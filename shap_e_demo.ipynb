{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "IvGwBwH33ppf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/shap-e.git\n",
        "%cd shap-e\n",
        "!pip install -e .\n",
        "!pip install numpy-stl\n",
        "!pip install --upgrade diffusers transformers accelerate peft rembg pyvista fast-simplification"
      ],
      "metadata": {
        "id": "A-h8mlw4bmVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9v4FkqD5O18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate Image"
      ],
      "metadata": {
        "id": "HsHV_wL43ytt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import LCMScheduler, AutoPipelineForText2Image\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "adapter_id = \"latent-consistency/lcm-lora-sdxl\"\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "# load and fuse lcm lora\n",
        "pipe.load_lora_weights(adapter_id)\n",
        "pipe.fuse_lora()\n",
        "\n",
        "prompt = \"a yellow mug\"\n",
        "\n",
        "# disable guidance_scale by passing 0\n",
        "image = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]"
      ],
      "metadata": {
        "id": "6JeD6xlJ34Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove the Background"
      ],
      "metadata": {
        "id": "lDXOrf596XHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rembg"
      ],
      "metadata": {
        "id": "lCE79nsG6Z3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rembg import remove, new_session\n",
        "model_name = \"u2netp\"\n",
        "session = new_session(model_name)\n",
        "output = remove(image, bgcolor=(255, 255, 255, 1), session=session)"
      ],
      "metadata": {
        "id": "ASQ7JOvN6h4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "46mTR6X87W-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate 3D model"
      ],
      "metadata": {
        "id": "nERdkmMK3sfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from shap_e.diffusion.sample import sample_latents\n",
        "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
        "from shap_e.models.download import load_model, load_config\n",
        "from shap_e.util.image_util import load_image\n",
        "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget, decode_latent_mesh\n"
      ],
      "metadata": {
        "id": "7JYaAPPAeMqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "tJ2tSAHieNmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xm = load_model('transmitter', device=device)\n",
        "model = load_model('image300M', device=device)\n",
        "diffusion = diffusion_from_config(load_config('diffusion'))"
      ],
      "metadata": {
        "id": "HX7Y7wB-ePGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "guidance_scale = 3.0\n",
        "\n",
        "latents = sample_latents(\n",
        "    batch_size=batch_size,\n",
        "    model=model,\n",
        "    diffusion=diffusion,\n",
        "    guidance_scale=guidance_scale,\n",
        "    model_kwargs=dict(images=[output] * batch_size),\n",
        "    progress=True,\n",
        "    clip_denoised=True,\n",
        "    use_fp16=True,\n",
        "    use_karras=True,\n",
        "    karras_steps=64,\n",
        "    sigma_min=1e-3,\n",
        "    sigma_max=160,\n",
        "    s_churn=0,\n",
        ")"
      ],
      "metadata": {
        "id": "3J5nu7y-eQ0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvista\n",
        "import fast_simplification\n",
        "import os\n",
        "render_mode = 'nerf' # you can change this to 'stf' for mesh rendering\n",
        "size = 64 # this is the size of the renders; higher values take longer to render.\n",
        "\n",
        "cameras = create_pan_cameras(size, device)\n",
        "\n",
        "for i, latent in enumerate(latents):\n",
        "    t = decode_latent_mesh(xm, latent).tri_mesh()\n",
        "    with open(f'example_mesh_{i}.obj', 'w') as f:\n",
        "        t.write_obj(f)\n",
        "\n",
        "for i, latent in enumerate(latents):\n",
        "  mesh = pyvista.read(f'example_mesh_{i}.obj')\n",
        "  simple = fast_simplification.simplify_mesh(mesh, 0.3)\n",
        "  simple.save(f'example_mesh_{i}.stl')\n"
      ],
      "metadata": {
        "id": "SeUfbrQ5eTDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UG3xcnUQ-iRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "cjxMrar6CzNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy-stl"
      ],
      "metadata": {
        "id": "jaSd8w0KC8x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stl import mesh\n",
        "from mpl_toolkits import mplot3d\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Create a new plot\n",
        "figure = pyplot.figure()\n",
        "axes = figure.add_subplot(projection='3d')\n",
        "\n",
        "# Load the STL files and add the vectors to the plot\n",
        "your_mesh = mesh.Mesh.from_file('/content/shap-e/example_mesh_3.stl')\n",
        "axes.add_collection3d(mplot3d.art3d.Poly3DCollection(your_mesh.vectors))\n",
        "\n",
        "# Auto scale to the mesh size\n",
        "scale = your_mesh.points.flatten()\n",
        "axes.auto_scale_xyz(scale, scale, scale)\n",
        "\n",
        "# Show the plot to the screen\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "9nrtx0Y8FVSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}